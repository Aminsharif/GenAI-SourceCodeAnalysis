{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\GenAI\\\\Bappy\\\\LiveProject\\\\GenAI-SourceCodeAnalysis\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir rest_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"rest_repo/\"\n",
    "repo = Repo.clone_from(\"https://github.com/Aminsharif/GenAI-MedicalChatbot.git\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=['.py'],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_spliter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n",
    "                                                           chunk_size = 500,\n",
    "                                                           chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = doc_spliter.split_documents(documents=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'rest_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom langchain_groq import ChatGroq\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain.chains import create_retrieval_chain\\nfrom flask import Flask, render_template, jsonify, request\\nfrom src.helper import load_huggingface_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom src.prompt import *\\nfrom dotenv import load_dotenv\\n\\napp = Flask(__name__)\\n\\nload_dotenv()'),\n",
       " Document(metadata={'source': 'rest_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='load_dotenv()\\n\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\ngroq_api_key = os.getenv(\\'GROQ_API_KEY\\')\\n\\n\\nembeddings = load_huggingface_embeddings()\\nllm = ChatGroq(model_name = \"llama3-8b-8192\",temperature=0.5,max_tokens=500, groq_api_key = os.getenv(\\'GROQ_API_KEY\\'))\\n\\nindex_name = \\'medicalbot\\'\\n\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name = index_name,\\n    embedding = embeddings\\n)\\n\\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})'),\n",
       " Document(metadata={'source': 'rest_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"question_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n@app.route('/')\"),\n",
       " Document(metadata={'source': 'rest_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def index():\\n    return render_template(\\'chat.html\\')\\n\\n@app.route(\"/get\", methods = [\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\\'msg\\']\\n    input = msg\\n    print(input)\\n    print(\\'........................\\')\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\\'response: \\', response[\\'answer\\'])\\n    print(\\'_________________________________\\')\\n    return str(response[\\'answer\\'])\\n\\n\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)'),\n",
       " Document(metadata={'source': 'rest_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'Generative AI Chatbot Project',\\n    version= '0.0.1',\\n    author= 'Amin Sharif',\\n    author_email= 'sharifmia236@gmail.com',\\n    packages= find_packages(),\\n    install_requires = []\\n\\n)\"),\n",
       " Document(metadata={'source': 'rest_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_spliter, load_huggingface_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nimport os\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nPINECONE_API_KEY = os.getenv(\\'PINECONE_API_KEY\\')\\n\\ndata=load_pdf_file(data=\\'Data/\\')\\ndocuments=text_spliter(data)\\nembeddings = load_huggingface_embeddings()\\n\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\nindex_name = \"medicalbot\"'),\n",
       " Document(metadata={'source': 'rest_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='pc.create_index(\\n    name=index_name,\\n    dimension=384,\\n    metric=\"cosine\",\\n    spec=ServerlessSpec(\\n        cloud=\"aws\",\\n        region=\"us-east-1\"\\n    )\\n)\\n\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=documents,\\n    index_name = index_name,\\n    embedding=embeddings\\n)'),\n",
       " Document(metadata={'source': 'rest_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n    \"test.py\"\\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)'),\n",
       " Document(metadata={'source': 'rest_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"creating directory; {filedir} for thd file: {filename}\")\\n    if(not os.path.exists(filepath)) or (os.path.getsize(filepath)==0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Createing empty file: {filepath}\")\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'rest_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\ndef load_pdf_file(path):\\n    loader = DirectoryLoader(path,\\n                             glob=\"*.pdf\",\\n                             loader_cls=PyPDFLoader)\\n    data = loader.load()\\n    return data'),\n",
       " Document(metadata={'source': 'rest_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='def text_spliter(data):\\n    text_spliter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\\n    text_chunks = text_spliter.split_documents(data)\\n    return text_chunks\\n\\ndef load_huggingface_embeddings():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings'),\n",
       " Document(metadata={'source': 'rest_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.prompts import ChatPromptTemplate\\n\\n\\nsystem_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\\'human\\', \\'{input}\\')\\n    ]\\n)')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sharif\\AppData\\Local\\Temp\\ipykernel_9536\\3409896792.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=\"./db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model_name='llama3-8b-8192', temperature=0.5, groq_api_key= groq_api_key, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type = \"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is load_hugging_face_embeddings function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `load_huggingface_embeddings` function is used to load pre-trained language model embeddings from the Hugging Face Transformers library. The function returns an instance of the `HuggingFaceEmbeddings` class, which is a wrapper around the pre-trained model.\n",
      "\n",
      "The function takes the model name as an argument, which specifies the pre-trained model to load. In this case, the model name is `\"sentence-transformers/all-MiniLM-L6-v2\"`, which is a pre-trained sentence transformer model.\n",
      "\n",
      "The `load_huggingface_embeddings` function is used to load the pre-trained embeddings for the purpose of generating vector representations of text documents. These embeddings are then used to index and search the documents in the Pinecone vector store.\n",
      "\n",
      "Here is the code snippet for the `load_huggingface_embeddings` function:\n",
      "```\n",
      "def load_huggingface_embeddings():\n",
      "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "    return embeddings\n",
      "```\n",
      "This function is used to load the pre-trained embeddings and return them as an instance of the `HuggingFaceEmbeddings` class.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
